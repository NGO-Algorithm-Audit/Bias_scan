# Fairness through discussion: A deliberative way forward

The main deliverable of this submission is the AI Auditing Report: [link]

Structure of this repository.

```
    .
    ├── bias_scan_notebooks     # Quantitative bias scan methods 
    ├── data                    # Data sets
    ├── images                  # Images used in notebook and report
    ├── literature              # Reference materials
    ├── mdss                    # AIF360 bias scan method
    ├── report                  # Main document
    ├── src                     # Source files for scripts (e.g., helper functions)
    ├── .gitignore              # files to be ignored in this repo
    ├── LICENSE                 # MIT license for sharing 
    ├── README.md               # Read me file
    └── requirements.txt        # Requirements for reproduce coding examples
```

## Summary
Discrimination is context-dependent [...]. Applying non-discrimination requirements in practice, one runs into difficulties: Under what circumstances proxy-variables for protected characteristics, such as ethnicity, can justifiably be used? Which metrics can be used for measuring the fairness of an algorithm? And: How we can arrive at well-founded quantitative thresholds? There is hence a gap between the qualitative requirements of law and ethics, and the quantitative nature of AI. Algorithm Audit aims to bridge this gap by letting a commission of experts mediate between these aspects for a concrete case, taking into account its specific social, institutional and technical context.

## Contributors
- Jurriaan Parie, Trustworthy AI data scientist at IBM
- Ariën Voogt, PhD-candidate in Philosophy at Protestant Theological University of Amsterdam
- dr. Vahid Niamadpour, PhD-candidate in Linguistics at Leiden University