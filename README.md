# Fairness through discussion: A deliberative way forward

The main deliverable of this submission is the [NGO Algorithm Audit Report](https://github.com/NGO-Algorithm-Audit/AI_Audit_Challenge/blob/master/report/Report_AI_Audit_Challenge.docx)

Structure of this repository.

```
    .
    ├── bias_scan_notebooks     # Quantitative bias scan methods 
    ├── data                    # Data sets
    ├── images                  # Images used in notebook and report
    ├── literature              # Reference materials
    ├── report                  # Main document
    ├── .gitattributes          # To store large files
    ├── .gitignore              # Files to be ignored in this repo
    ├── LICENSE                 # MIT license for sharing 
    ├── README.md               # Read me file
    └── requirements.txt        # Requirements for reproduce coding examples
```

## Summary
Discrimination is context-dependent [...]. Applying non-discrimination requirements in practice, one runs into difficulties: Under what circumstances proxy-variables for protected characteristics, such as ethnicity, can justifiably be used? Which metrics can be used for measuring the fairness of an algorithm? And: How we can arrive at well-founded quantitative thresholds? There is hence a gap between the qualitative requirements of law and ethics, and the quantitative nature of AI. Algorithm Audit aims to bridge this gap by letting a commission of experts mediate between these aspects for a concrete case, taking into account its specific social, institutional and technical context.

## Contributors
- Jurriaan Parie, Trustworthy AI data scientist at IBM
- Ariën Voogt, PhD-candidate in Philosophy at Protestant Theological University of Amsterdam
- dr. Vahid Niamadpour, PhD-candidate in Linguistics at Leiden University