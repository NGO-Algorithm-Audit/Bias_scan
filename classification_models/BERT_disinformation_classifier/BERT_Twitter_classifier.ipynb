{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5aab57d",
   "metadata": {},
   "source": [
    "## BERT Twitter disinformation classifier\n",
    "In this notebooks a disinformation classifier for tweets is trained using a pre-trained BERT natural language processing model. Overview: \n",
    "1. Loading data\n",
    "2. Explanatory data analysis\n",
    "3. Data preparations\n",
    "4. Tokenization\n",
    "5. Fine-tune pre-trained BERT model and model predictions\n",
    "6. Model perfomance on test data set\n",
    "\n",
    "To build this classifier online tutorials were consulted, among others https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ff961",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c714bf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import torch\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# BERT transformers\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# notebook/markdown\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Tensorflow/Keras tokenizer\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "\n",
    "# torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# tqdm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5aeb5",
   "metadata": {},
   "source": [
    "### 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79587909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#followers</th>\n",
       "      <th>user_engagement</th>\n",
       "      <th>verified</th>\n",
       "      <th>depth</th>\n",
       "      <th>user_id1</th>\n",
       "      <th>tweet_id1</th>\n",
       "      <th>user_id2</th>\n",
       "      <th>length</th>\n",
       "      <th>#hashs</th>\n",
       "      <th>#mentions</th>\n",
       "      <th>#URLs</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15375121</td>\n",
       "      <td>72.567469</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>489800427152879616</td>\n",
       "      <td>2467791</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>malaysia airlines says it lost contact with pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3673898</td>\n",
       "      <td>55.294333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>560474897013415936</td>\n",
       "      <td>59553554</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>for just $1 you can get a free jr. frosty with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1274260</td>\n",
       "      <td>32.033388</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>524928119955013632</td>\n",
       "      <td>19038934</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>police say they have located car belonging to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13955752</td>\n",
       "      <td>64.548896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>518830518792892416</td>\n",
       "      <td>51241574</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>mexico security forces hunting 43 missing stud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189683</td>\n",
       "      <td>24.726166</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>551117430345711616</td>\n",
       "      <td>2280470022</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>news saudi arabia's national airline planning ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #followers  user_engagement  verified  depth user_id1           tweet_id1  \\\n",
       "0    15375121        72.567469         1    0.0     ROOT  489800427152879616   \n",
       "1     3673898        55.294333         1    0.0     ROOT  560474897013415936   \n",
       "2     1274260        32.033388         1    0.0     ROOT  524928119955013632   \n",
       "3    13955752        64.548896         1    0.0     ROOT  518830518792892416   \n",
       "4      189683        24.726166         1    0.0     ROOT  551117430345711616   \n",
       "\n",
       "     user_id2  length  #hashs  #mentions  #URLs  sentiment_score  \\\n",
       "0     2467791      95       0          0      2          -0.3182   \n",
       "1    59553554     118       0          1      1           0.8398   \n",
       "2    19038934     133       1          0      0          -0.7269   \n",
       "3    51241574      96       0          0      1          -0.3400   \n",
       "4  2280470022      96       0          0      2           0.0000   \n",
       "\n",
       "                                                text  label  \n",
       "0  malaysia airlines says it lost contact with pl...      1  \n",
       "1  for just $1 you can get a free jr. frosty with...      1  \n",
       "2  police say they have located car belonging to ...      0  \n",
       "3  mexico security forces hunting 43 missing stud...      1  \n",
       "4  news saudi arabia's national airline planning ...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../../data/Twitter_dataset/twitter_full.csv'\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685756b1",
   "metadata": {},
   "source": [
    "### 2. Explanatory data analysis \n",
    "#### Lengths of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e2804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 741\n",
      "Max length of tweet: 140\n",
      "Mean length of tweets: 91.59784075573549\n"
     ]
    }
   ],
   "source": [
    "tweet_ls = [tweet for tweet in df.text]\n",
    "\n",
    "max_len = 0\n",
    "tweet_len = []\n",
    "for tweet in tweet_ls:\n",
    "    tweet_len.append(len(tweet))\n",
    "\n",
    "print('Number of tweets:', len(tweet_ls))\n",
    "print('Max length of tweet:', max(tweet_len))\n",
    "print('Mean length of tweets:', np.mean(tweet_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ad926",
   "metadata": {},
   "source": [
    "### 3. Data preperations\n",
    "#### Selection and randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f413242c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a pregnant woman has lost her eye after being ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp confirmed that it's splitting into two comp...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#walmart donates $10,000 to #darrenwilson fund...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that powerful letter from lego to parents from...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>currently boycotting walmart &amp; sam's on w. flo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  a pregnant woman has lost her eye after being ...      0\n",
       "1  hp confirmed that it's splitting into two comp...      0\n",
       "2  #walmart donates $10,000 to #darrenwilson fund...      1\n",
       "3  that powerful letter from lego to parents from...      0\n",
       "4  currently boycotting walmart & sam's on w. flo...      1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select source tweets and prediction labels\n",
    "df = df[['text','label']]\n",
    "\n",
    "# randomize data frame\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38dc44e",
   "metadata": {},
   "source": [
    "#### Split data intro train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3bbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape trainset: (474, 2)\n",
      "shape valset: (119, 2)\n",
      "shape testset: (148, 2)\n"
     ]
    }
   ],
   "source": [
    "# test set\n",
    "train_val_df = df.sample(frac = 0.8)\n",
    "test_df = df.drop(train_val_df.index)\n",
    "\n",
    "# train and validation set\n",
    "train_df = train_val_df.sample(frac = 0.8)\n",
    "val_df = train_val_df.drop(train_df.index)\n",
    "\n",
    "# reset index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print('shape trainset:', train_df.shape)\n",
    "print('shape valset:', val_df.shape)\n",
    "print('shape testset:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfc400",
   "metadata": {},
   "source": [
    "#### Export train, validation and test set to .tsv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f7e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./train.tsv', sep='\\t', index=False)\n",
    "# val_df.to_csv('./val.tsv', sep='\\t', index=False)\n",
    "# test_df.to_csv('./test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620dbde",
   "metadata": {},
   "source": [
    "#### Concatenate data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c201a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5ee28",
   "metadata": {},
   "source": [
    "#### Data Cleaning: Removing stopwords from all source tweets (train, validation and test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49fa0e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jurriaanparie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowload stopwords from nltk library\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d06ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords and words with ≤2 characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96b05b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load stopwords from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# cleaning source tweets\n",
    "df['cleaned'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b5d6e",
   "metadata": {},
   "source": [
    "### Words in dataset after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8391342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_ls = []\n",
    "for i in df.cleaned:\n",
    "    for j in i:\n",
    "        words_ls.append(j)\n",
    "\n",
    "words = len(list(set(words_ls)))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee7b95",
   "metadata": {},
   "source": [
    "### 4. Tokenization\n",
    "#### Tokenize source tweets using Tensorflow's one-hot tokenization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b14fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize source tweets \n",
    "tokenizer = Tokenizer(num_words = words)\n",
    "tokenizer.fit_on_texts(train_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0fa0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequence of tokenized words for train, validation and test set \n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70c87c",
   "metadata": {},
   "source": [
    "### 5. Fine-tune pre-trained BERT model\n",
    "#### Load pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbc5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7481fe",
   "metadata": {},
   "source": [
    "#### Create class for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79040569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv('./' + mode + '.tsv', sep='\\t').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        else:\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        word_pieces = ['[CLS]']\n",
    "        statement = self.tokenizer.tokenize(statement)\n",
    "        word_pieces += statement + ['[SEP]']\n",
    "        len_st = len(word_pieces)\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987fff7",
   "metadata": {},
   "source": [
    "#### Initialize train, validation and test data set for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65c132be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 474\n",
      "valset size: 119\n",
      "testset size:  148\n"
     ]
    }
   ],
   "source": [
    "# Initialize Datasets for Transformation\n",
    "train_set = TwitterDataset('train', tokenizer=tokenizer)\n",
    "val_set = TwitterDataset('val', tokenizer=tokenizer)\n",
    "test_set = TwitterDataset('test', tokenizer=tokenizer)\n",
    "\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fba89",
   "metadata": {},
   "source": [
    "#### Initialize sampling and observing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9810d79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original_statement: \n",
      "remembering paul walker: fast and furious star was \"so happy\" hours before his tragic death URL\n",
      "\n",
      "tokens: \n",
      "['[CLS]', 'remembering', 'paul', 'walker', ':', 'fast', 'and', 'furious', 'star', 'was', '\"', 'so', 'happy', '\"', 'hours', 'before', 'his', 'tragic', 'death', 'ur', '##l', '[SEP]']\n",
      "\n",
      "label: 0\n",
      "\n",
      "--------------------\n",
      "\n",
      "tokens_tensor: \n",
      "tensor([  101, 10397,  2703,  5232,  1024,  3435,  1998,  9943,  2732,  2001,\n",
      "         1000,  2061,  3407,  1000,  2847,  2077,  2010, 13800,  2331, 24471,\n",
      "         2140,   102])\n",
      "\n",
      "segments_tensor: \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "label_tensor: \n",
      "0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "statement, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "\n",
    "print(f\"\"\"\n",
    "original_statement: \n",
    "{statement}\n",
    "\n",
    "tokens: \n",
    "{tokens}\n",
    "\n",
    "label: {label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "tokens_tensor: \n",
    "{tokens_tensor}\n",
    "\n",
    "segments_tensor: \n",
    "{segments_tensor}\n",
    "\n",
    "label_tensor: \n",
    "{label_tensor}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637177b",
   "metadata": {},
   "source": [
    "#### Transforming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e860280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero padding\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "valloader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "testloader = DataLoader(test_set, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "889dbdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 30]) \n",
      "tensor([[  101, 10397,  2703,  5232,  1024,  3435,  1998,  9943,  2732,  2001,\n",
      "          1000,  2061,  3407,  1000,  2847,  2077,  2010, 13800,  2331, 24471,\n",
      "          2140,   102,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 10651,  1024,  8443,  3003,  1999,  2680,  2655,  1024,  2057,\n",
      "          2074,  2915,  2091,  1037,  4946,  1006,  2394,  4942, 27430,  1007,\n",
      "         24471,  2140,  1001,  1049,  2232, 16576, 24471,  2140,   102,     0],\n",
      "        [  101,  3516,  2450, 12778,  1002,  2322,  1010,  2199,  2005,  2353,\n",
      "          7388,  1064, 24471,  2140, 24471,  2140,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1001,  1049,  2232, 16576,  2003,  3373,  2915,  2091,  2007,\n",
      "          2214,  3354,  1011,  2328, 20934,  2243,  3302,  1011,  2000,  1011,\n",
      "          2250,  7421,  2291,  1012, 24471,  2140, 24471,  2140,   102,     0],\n",
      "        [  101,  2008,  3928, 23853,  3661,  2000,  3008,  2013,  1996,  3955,\n",
      "          1029,  2009,  1005,  1055,  2613, 24471,  2140, 24471,  2140,   102,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2437,  2026,  2219,  2079, 28414,  2015, 14894,  2098,  3137,\n",
      "         24903,   999, 24471,  2140,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6522,  4484,  2008,  2009,  1005,  1055, 14541,  2046,  2048,\n",
      "          3316,  1012,  1998,  6276,  1019,  1010,  2199,  5841,  1012, 24471,\n",
      "          2140,  1002,  6522, 24471,  2140,   102,     0,     0,     0,     0],\n",
      "        [  101,  2092,  1010,  2023,  2003,  2028,  2126,  2000,  8046,  2115,\n",
      "          3105,  1012,  3422,  7397,  2078,  6398, 25869,  4135,  2665,  8046,\n",
      "          2444,  2006,  2250,  1012, 24471,  2140, 24471,  2140,   102,     0],\n",
      "        [  101,  1001, 22052, 15794,  9072,  4710, 12411,  1012,  2726, 24003,\n",
      "          1010,  2005,  3038,  2308,  2024,  1000,  1037,  8276,  3013,  1997,\n",
      "          6240,  1000, 24471,  2140, 24471,  2140,   102,     0,     0,     0],\n",
      "        [  101,  8495, 28664,  6884,  5746,  1997,  2745,  2829,  5008, 24471,\n",
      "          2140, 24471,  2140,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  3986,  2005,  1001,  1049,  2232, 16576,  5694,  3596,\n",
      "          2379,  1996,  4549,  8408,  1999, 12100,  1024, 24471,  2140,  1006,\n",
      "          3142, 14255, 15000,  3240,  1013, 19044,  1007, 24471,  2140,   102],\n",
      "        [  101,  1996,  2088,  2003,  2770,  2041,  1997,  7967,  1010,  2088,\n",
      "          1521,  1055,  2922,  7967,  7751, 19428, 24471,  2140, 24471,  2140,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  5085,  2100,  1005,  1055,  7203,  3433,  2000,  1996,  4918,\n",
      "          2002,  2497,  3527,  2886,  3475,  1005,  1056,  2011,  5085,  2100,\n",
      "          1012,  2021,  2009,  2003,  8478, 24471,  2140, 24471,  2140,   102],\n",
      "        [  101,  4205,  2057,  2323,  2079,  1037,  3231,  2007,  2023,  6804,\n",
      "           999,  1012,  1012,  1012,  4205,  1029, 24471,  2140, 24471,  2140,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  2175,  6632,  2705,  8087,  1024, 17022,  1011,  7451,  6804,\n",
      "         20096,  7155,  1999, 18951,  1030,  4562, 16523,  8516,  4877,  1030,\n",
      "          7090, 24471,  2140, 24471,  2140,   102,     0,     0,     0,     0],\n",
      "        [  101,  1037,  6302,  1997,  2304, 11500,  7494,  1996,  2166,  1997,\n",
      "          1037,  1047, 19658,  2266,  1012, 24471,  2140,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 30])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 30])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([16])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dab218",
   "metadata": {},
   "source": [
    "#### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17531b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name             module\n",
      "-----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout          Dropout(p=0.1, inplace=False)\n",
      "classifier       Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name             module\n",
    "-----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:16} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "216d6438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa1fa1",
   "metadata": {},
   "source": [
    "#### Fine-tuning BERT and make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ffb78039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f65d797d3fd468b9e9eac5c11407c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7503f4e7265e474cb159e32faa547f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fc9676e2fb4447ad6a0ee03ec34602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    loop = tqdm(trainloader)\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc = accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(acc = train_acc, loss = train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c086c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, './best_model.pth')\n",
    "# print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286440f",
   "metadata": {},
   "source": [
    "### 6. Model performance on test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "594ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc:  0.8648648648648649\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            data = [t.to(device) for t in data if t is not None]\n",
    "            \n",
    "        tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "        test_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors)\n",
    "\n",
    "        logits = test_outputs[0]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "        labels = data[3]\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true, predictions, labels=[1, 0], normalize='pred')\n",
    "\n",
    "cmap0 = mpl.colors.LinearSegmentedColormap.from_list(\n",
    "        'unevently divided', ['#618EC7','#fffde4'])\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['True', 'False'])\n",
    "disp.plot(cmap=cmap0)\n",
    "\n",
    "print('Acc: ', accuracy_score(predictions,true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae2d45",
   "metadata": {},
   "source": [
    "#### Dataframe with ground truth and predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a40c200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marlboro man dies of smoking-related disease U...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oklahoma prosecutor files formal notification...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#mexico: doubts grow over the fate of 43 missi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>who killed deandre joshua - and why? still mor...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i feel some type of way. URL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  pred_label\n",
       "0  marlboro man dies of smoking-related disease U...      0           1\n",
       "1  #oklahoma prosecutor files formal notification...      0           0\n",
       "2  #mexico: doubts grow over the fate of 43 missi...      0           0\n",
       "3  who killed deandre joshua - and why? still mor...      1           0\n",
       "4                       i feel some type of way. URL      1           1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"pred_label\": predictions})\n",
    "df_true = pd.DataFrame({\"label\": true})\n",
    "\n",
    "df_pred = pd.concat([test_df.loc[:, ['text']], \n",
    "                     df_true.loc[:, ['label']], \n",
    "                     df.loc[:, 'pred_label']], axis=1)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a178d4f",
   "metadata": {},
   "source": [
    "#### Export model predictions to `.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b718161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('./pred_BERT.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
