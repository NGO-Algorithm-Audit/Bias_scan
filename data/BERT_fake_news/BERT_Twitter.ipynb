{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d52ff961",
   "metadata": {},
   "source": [
    "### Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c714bf0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>#followers</th>\n",
       "      <th>user_engagement</th>\n",
       "      <th>verified</th>\n",
       "      <th>depth</th>\n",
       "      <th>user_id1</th>\n",
       "      <th>tweet_id1</th>\n",
       "      <th>t1</th>\n",
       "      <th>user_id2</th>\n",
       "      <th>t2</th>\n",
       "      <th>veracity</th>\n",
       "      <th>resp_time</th>\n",
       "      <th>retw_prob</th>\n",
       "      <th>length</th>\n",
       "      <th>#hashs</th>\n",
       "      <th>#mentions</th>\n",
       "      <th>#URLs</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>489800427152879616</td>\n",
       "      <td>15375121</td>\n",
       "      <td>72.567469</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>489800427152879616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2467791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>malaysia airlines says it lost contact with pl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>560474897013415936</td>\n",
       "      <td>3673898</td>\n",
       "      <td>55.294333</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>560474897013415936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59553554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8398</td>\n",
       "      <td>for just $1 you can get a free jr. frosty with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>524928119955013632</td>\n",
       "      <td>1274260</td>\n",
       "      <td>32.033388</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>524928119955013632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19038934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.7269</td>\n",
       "      <td>police say they have located car belonging to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>518830518792892416</td>\n",
       "      <td>13955752</td>\n",
       "      <td>64.548896</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>518830518792892416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51241574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3400</td>\n",
       "      <td>mexico security forces hunting 43 missing stud...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>551117430345711616</td>\n",
       "      <td>189683</td>\n",
       "      <td>24.726166</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>551117430345711616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2280470022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>news saudi arabia's national airline planning ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_name  #followers  user_engagement  verified  depth user_id1  \\\n",
       "0  489800427152879616    15375121        72.567469         1    0.0     ROOT   \n",
       "1  560474897013415936     3673898        55.294333         1    0.0     ROOT   \n",
       "2  524928119955013632     1274260        32.033388         1    0.0     ROOT   \n",
       "3  518830518792892416    13955752        64.548896         1    0.0     ROOT   \n",
       "4  551117430345711616      189683        24.726166         1    0.0     ROOT   \n",
       "\n",
       "            tweet_id1   t1    user_id2   t2  veracity  resp_time  retw_prob  \\\n",
       "0  489800427152879616  0.0     2467791  0.0         0        0.0   0.000031   \n",
       "1  560474897013415936  0.0    59553554  0.0         0        0.0   0.000032   \n",
       "2  524928119955013632  0.0    19038934  0.0         1        0.0   0.000184   \n",
       "3  518830518792892416  0.0    51241574  0.0         0        0.0   0.000007   \n",
       "4  551117430345711616  0.0  2280470022  0.0         0        0.0   0.000617   \n",
       "\n",
       "   length  #hashs  #mentions  #URLs  sentiment_score  \\\n",
       "0      95       0          0      2          -0.3182   \n",
       "1     118       0          1      1           0.8398   \n",
       "2     133       1          0      0          -0.7269   \n",
       "3      96       0          0      1          -0.3400   \n",
       "4      96       0          0      2           0.0000   \n",
       "\n",
       "                                                text  label  \n",
       "0  malaysia airlines says it lost contact with pl...      1  \n",
       "1  for just $1 you can get a free jr. frosty with...      1  \n",
       "2  police say they have located car belonging to ...      0  \n",
       "3  mexico security forces hunting 43 missing stud...      1  \n",
       "4  news saudi arabia's national airline planning ...      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = './Twitter_data/twitter_full.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685756b1",
   "metadata": {},
   "source": [
    "### Lengths of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e2804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 741\n",
      "Max length of tweet: 140\n",
      "Mean length of tweets: 91.59784075573549\n"
     ]
    }
   ],
   "source": [
    "tweet_ls = [tweet for tweet in df.text]\n",
    "\n",
    "max_len = 0\n",
    "tweet_len = []\n",
    "for tweet in tweet_ls:\n",
    "    tweet_len.append(len(tweet))\n",
    "\n",
    "print('Number of tweets:', len(tweet_ls))\n",
    "print('Max length of tweet:', max(tweet_len))\n",
    "print('Mean length of tweets:', np.mean(tweet_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449ad926",
   "metadata": {},
   "source": [
    "###  Data preperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56852cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f413242c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apec photo of the day. rt @marc_leibowitz: pho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>latest update: attacker shot dead in parliamen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>developing news: soldier shot at war memorial....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>breaking: malaysian airlines passenger 'shot d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cdc whistleblower exposes ebola vaccinations c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>hp confirms that it's splitting into two compa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>goliath encounter: puppy-sized spider surprise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>nbc: arrest records show #ferguson cops jailed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>new. leaked phone call between rebel leader &amp; ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>gonna be unreal when las vegas goes undefeated...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    apec photo of the day. rt @marc_leibowitz: pho...      1\n",
       "1    latest update: attacker shot dead in parliamen...      0\n",
       "2    developing news: soldier shot at war memorial....      0\n",
       "3    breaking: malaysian airlines passenger 'shot d...      1\n",
       "4    cdc whistleblower exposes ebola vaccinations c...      1\n",
       "..                                                 ...    ...\n",
       "736  hp confirms that it's splitting into two compa...      0\n",
       "737  goliath encounter: puppy-sized spider surprise...      0\n",
       "738  nbc: arrest records show #ferguson cops jailed...      1\n",
       "739  new. leaked phone call between rebel leader & ...      1\n",
       "740  gonna be unreal when las vegas goes undefeated...      1\n",
       "\n",
       "[741 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle\n",
    "df = shuffle(df).reset_index(drop=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38dc44e",
   "metadata": {},
   "source": [
    "### Split data intro train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3bbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: (474, 2)\n",
      "valset size: (119, 2)\n",
      "testset size: (148, 2)\n"
     ]
    }
   ],
   "source": [
    "train_val_df = df.sample(frac = 0.8)\n",
    "test_df = df.drop(train_val_df.index)\n",
    "\n",
    "train_df = train_val_df.sample(frac = 0.8)\n",
    "val_df = train_val_df.drop(train_df.index)\n",
    "\n",
    "# Reset Index\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "print('trainset size:', train_df.shape)\n",
    "print('valset size:', val_df.shape)\n",
    "print('testset size:', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfc400",
   "metadata": {},
   "source": [
    "### Dataframe to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f7e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv('./train.tsv', sep='\\t', index=False)\n",
    "# val_df.to_csv('./val.tsv', sep='\\t', index=False)\n",
    "# test_df.to_csv('./test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620dbde",
   "metadata": {},
   "source": [
    "### Concatenate all dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c201a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vladimir putin’s motorcade as seen from the ai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>high school student reportedly makes 72 millio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>banksy's response to the #charliehebdo attack ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>officials took away this halloween decoration ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>audio may have captured michael brown shooting...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>canada parliament shooting: - active shooter -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>fbi probing alleged audio of michael brown sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>madden nfl 15 not only predicted a patriots vi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>scene at ottawa war memorial. soldier on duty ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>goliath encounter: puppy-sized spider surprise...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>741 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    vladimir putin’s motorcade as seen from the ai...      1\n",
       "1    high school student reportedly makes 72 millio...      1\n",
       "2    banksy's response to the #charliehebdo attack ...      1\n",
       "3    officials took away this halloween decoration ...      0\n",
       "4    audio may have captured michael brown shooting...      0\n",
       "..                                                 ...    ...\n",
       "143  canada parliament shooting: - active shooter -...      0\n",
       "144  fbi probing alleged audio of michael brown sho...      0\n",
       "145  madden nfl 15 not only predicted a patriots vi...      0\n",
       "146  scene at ottawa war memorial. soldier on duty ...      0\n",
       "147  goliath encounter: puppy-sized spider surprise...      0\n",
       "\n",
       "[741 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train_df, val_df, test_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc5ee28",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49fa0e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jurriaanparie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Downloading Stopwords\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29a13e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining Additional Stopwords From nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d06ba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords And Remove Words With 2 Or Less Characters\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96b05b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Applying The Function To The Dataframe\n",
    "df['clean'] = df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80b5d6e",
   "metadata": {},
   "source": [
    "### Total words in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8391342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2046"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_words = []\n",
    "for i in df.clean:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)\n",
    "\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee7b95",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cd10659",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b30e41",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b14fb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "\n",
    "# Creating A Tokenizer To Tokenize The Words And Create Sequences Of Tokenized Words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c70c87c",
   "metadata": {},
   "source": [
    "### Fine-tuning pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40ba6298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fbc5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7481fe",
   "metadata": {},
   "source": [
    "### Dataset as Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79040569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset size: 474\n",
      "valset size: 119\n",
      "testset size:  148\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "        self.mode = mode\n",
    "        self.df = pd.read_csv('./' + mode + '.tsv', sep='\\t').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        self.tokenizer = tokenizer  # BERT tokenizer\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'test':\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "        else:\n",
    "            statement, label = self.df.iloc[idx, :].values\n",
    "            label_tensor = torch.tensor(label)\n",
    "            \n",
    "        word_pieces = ['[CLS]']\n",
    "        statement = self.tokenizer.tokenize(statement)\n",
    "        word_pieces += statement + ['[SEP]']\n",
    "        len_st = len(word_pieces)\n",
    "        \n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# Initialize Datasets for Transformation\n",
    "trainset = FakeNewsDataset('train', tokenizer=tokenizer)\n",
    "valset = FakeNewsDataset('val', tokenizer=tokenizer)\n",
    "testset = FakeNewsDataset('test', tokenizer=tokenizer)\n",
    "\n",
    "print('trainset size:' ,trainset.__len__())\n",
    "print('valset size:',valset.__len__())\n",
    "print('testset size: ',testset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fba89",
   "metadata": {},
   "source": [
    "### Sampling and observing tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9810d79",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original_statement: \n",
      "it ain't banksy! meet @lucilleclerc, artist behind the iconic #charliehebdo pencil tribute URL URL\n",
      "\n",
      "tokens: \n",
      "['[CLS]', 'it', 'ain', \"'\", 't', 'banks', '##y', '!', 'meet', '@', 'lucille', '##cle', '##rc', ',', 'artist', 'behind', 'the', 'iconic', '#', 'charlie', '##he', '##b', '##do', 'pencil', 'tribute', 'ur', '##l', 'ur', '##l', '[SEP]']\n",
      "\n",
      "label: 1\n",
      "\n",
      "--------------------\n",
      "\n",
      "tokens_tensor: \n",
      "tensor([  101,  2009,  7110,  1005,  1056,  5085,  2100,   999,  3113,  1030,\n",
      "        28016, 14321, 11890,  1010,  3063,  2369,  1996, 14430,  1001,  4918,\n",
      "         5369,  2497,  3527, 14745,  7050, 24471,  2140, 24471,  2140,   102])\n",
      "\n",
      "segments_tensor: \n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0])\n",
      "\n",
      "label_tensor: \n",
      "1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 0\n",
    "\n",
    "statement, label = trainset.df.iloc[sample_idx].values\n",
    "\n",
    "tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "\n",
    "print(f\"\"\"\n",
    "original_statement: \n",
    "{statement}\n",
    "\n",
    "tokens: \n",
    "{tokens}\n",
    "\n",
    "label: {label}\n",
    "\n",
    "--------------------\n",
    "\n",
    "tokens_tensor: \n",
    "{tokens_tensor}\n",
    "\n",
    "segments_tensor: \n",
    "{segments_tensor}\n",
    "\n",
    "label_tensor: \n",
    "{label_tensor}\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637177b",
   "metadata": {},
   "source": [
    "### Reforming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e860280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # Zero Padding\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, batch_first=True)\n",
    "    \n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE,collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "889dbdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 35]) \n",
      "tensor([[  101,  2009,  7110,  1005,  1056,  5085,  2100,   999,  3113,  1030,\n",
      "         28016, 14321, 11890,  1010,  3063,  2369,  1996, 14430,  1001,  4918,\n",
      "          5369,  2497,  3527, 14745,  7050, 24471,  2140, 24471,  2140,   102,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2088,  2003,  2770,  2041,  1997,  7967,  1010,  2088,\n",
      "          1521,  1055,  2922,  7967,  7751, 19428, 24471,  2140, 24471,  2140,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101, 15885,  1024,  4901, 18571,  5312,  2044,  5008,  2757,  3010,\n",
      "          3323, 17346, 24471,  2140, 24471,  2140,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2703,  5232,  1005,  1055,  3435,  1998,  1996,  9943,  6849,\n",
      "          2869,  1998,  2062, 10509,  2000,  1996,  3364,  1005,  1055, 13800,\n",
      "          2331,  1024, 24471,  2140,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2703,  5232,  1005,  1055,  2839,  1999,  3435,  1998,  1996,\n",
      "          9943,  2001,  2315,  4422,  1000,  1010,  4422,  2013,  2155,  3124,\n",
      "          2036,  2351,  2023,  2733,  1010,  2119,  6677,  2920,  3765,  1012,\n",
      "          1000, 26114,   102,     0,     0],\n",
      "        [  101, 13723,  9266,  2078,  5829,  2000,  2031,  9147,  4848,  7219,\n",
      "          1012,  2074,  1999,  2051,  2005,  2014,  2047,  2338,  1000,  2129,\n",
      "          2000,  5660,  1037, 16522,  1000,  1012,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  3010,  3664,  4136,  5152,  2000,  9044, 15960,  1012, 10220,\n",
      "          2000,  6366, 15960,  2013,  2082, 26449,  1012, 24471,  2140, 24471,\n",
      "          2140,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2093,  1997,  2274, 26485, 29176,  2024,  2085, 18301,  4177,\n",
      "          4447, 24471,  2140, 24471,  2140, 24471,  2140,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  2619, 12509,  4993,  1037, 19085,  2006,  1037,  1002,  1016,\n",
      "          1012,  1018,  4971, 11829, 19321,  2072, 24471,  2140,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1037,  6302,  1997,  2304, 11500,  7494,  1996,  2166,  1997,\n",
      "          1037,  1047, 19658,  2266,  1012, 24471,  2140,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101, 14265,  7150, 25022, 24714,  2080,  1010,  2915,  2757,  2012,\n",
      "          1001,  8166,  2162,  3986,  2651,  1010,  2001,  2484,  1012, 24501,\n",
      "          2121, 11365,  2102,  2001,  2266,  1997, 27365,  1998, 14274, 22672,\n",
      "          1001,  3729, 16275, 10893,   102],\n",
      "        [  101,   100,  5499, 12152,  1999,   100,   100,   100,   100, 24471,\n",
      "          2140,  1030,  1035, 10424,  9956, 12733,  9956,  1035,  1030,  6864,\n",
      "          4168,  2243, 24471,  2140,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101, 10166,  1012,  8112,  4487, 11393,  2015,  2994,  2012,  2188,\n",
      "          3566,  2015,  1012,  1000,  2008,  1521,  1055,  2025,  1037,  3601,\n",
      "          2057,  2215,  4841,  2000,  2191,  1012,  1000, 24471,  2140,   102,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  9306,  2317,  2196,  3280,  2016,  2026,  3203,  2145,  1045,\n",
      "          2196,  2031,  1996,  3348,  2007,  2014,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  8112,  2038,  6406,  2976,  5571,  2114, 27946,  2005, 20084,\n",
      "         11851, 17789,  1005,  1055,  2942,  2916,  1012,  2643,  2003,  2204,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0],\n",
      "        [  101,  1005, 24890,  2321,  1005, 10173,  1996, 11579,  2052,  2663,\n",
      "          2654,  1011,  2484,  1012,  2029,  2003,  3599,  2054,  3047,  1012,\n",
      "         24471,  2140,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 35])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 35])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([16])\n",
      "tensor([1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dab218",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17531b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name             module\n",
      "-----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout          Dropout(p=0.1, inplace=False)\n",
      "classifier       Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(\"\"\"\n",
    "name             module\n",
    "-----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:16} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "216d6438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"_name_or_path\": \"bert-base-uncased\",\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aa1fa1",
   "metadata": {},
   "source": [
    "### Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffb78039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36efba588dad466db8cf4703f675940f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68edec0d92741e9b4a60b33bb4b9a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675756cebfb749108f2ae837e67b78a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    loop = tqdm(trainloader)\n",
    "    for batch_idx, data in enumerate(loop):\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        logits = outputs[1]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "        train_acc = accuracy_score(pred.cpu().tolist() , labels.cpu().tolist())\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(acc = train_acc, loss = train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c086c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, './best_model.pth')\n",
    "print('Model saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6286440f",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "594ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7752809  0.11864407]\n",
      " [0.2247191  0.88135593]]\n",
      "Acc:  0.8175675675675675\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "true=[]\n",
    "predictions=[]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in testloader:\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            data = [t.to(device) for t in data if t is not None]\n",
    "            \n",
    "        tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "        test_outputs = model(input_ids=tokens_tensors, \n",
    "                    token_type_ids=segments_tensors, \n",
    "                    attention_mask=masks_tensors)\n",
    "\n",
    "        logits = test_outputs[0]\n",
    "        _, pred = torch.max(logits.data, 1)\n",
    "\n",
    "        labels = data[3]\n",
    "        true.extend(labels.cpu().tolist())\n",
    "        predictions.extend(pred.cpu().tolist())\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true, predictions, labels=[1, 0], normalize='pred')\n",
    "print(cm)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])\n",
    "disp.plot()\n",
    "\n",
    "print('Acc: ', accuracy_score(predictions,true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a40c200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>apec photo of the day. rt @marc_leibowitz: pho...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>developing news: soldier shot at war memorial....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r.i.p to roger rodas who died alongside paul w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we are running out of chocolate, warns world's...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fact: florida state university shooter's last ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  pred_label\n",
       "0  apec photo of the day. rt @marc_leibowitz: pho...      0           0\n",
       "1  developing news: soldier shot at war memorial....      0           0\n",
       "2  r.i.p to roger rodas who died alongside paul w...      0           0\n",
       "3  we are running out of chocolate, warns world's...      0           0\n",
       "4  fact: florida state university shooter's last ...      0           1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\"pred_label\": predictions})\n",
    "df_true = pd.DataFrame({\"label\": true})\n",
    "\n",
    "df_pred = pd.concat([test_df.loc[:, ['text']], \n",
    "                     df_true.loc[:, ['label']], \n",
    "                     df.loc[:, 'pred_label']], axis=1)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b718161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.to_csv('./pred_bert_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
